name: Clean and Sanitize CSV

on:
  push:
    branches:
      - main
    paths:
      - 'feed-data.csv'
  pull_request:
    branches:
      - main
    paths:
      - 'feed-data.csv'

permissions:
  contents: write

jobs:
  clean-csv:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: 3.12.3

    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pandas beautifulsoup4 unidecode

    - name: Clean, sanitize, and sort CSV
      run: |
        python -c "
        import pandas as pd
        from bs4 import BeautifulSoup
        import logging
        from datetime import datetime
        from unidecode import unidecode

        # Configure logging
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

        def clean_title(title):
          if isinstance(title, str):
              # Remove HTML tags using BeautifulSoup
              soup = BeautifulSoup(title, 'html.parser')
              title = soup.get_text()
              # Remove leading/trailing whitespace
              title = title.strip()
              # Convert special characters to ASCII equivalents
              title = unidecode(title)
              return title
          else:
              return ''

        try:
            # Read the CSV file with UTF-8 encoding
            csv_file_path = 'feed-data.csv'
            df = pd.read_csv(csv_file_path, encoding='utf-8')

            # Clean and sanitize the 'Title' column
            df['Title'] = df['Title'].astype(str).apply(clean_title)

            # Remove duplicate entries based on the 'Link' column
            df = df.drop_duplicates(subset='Link', keep='first')

            # Handle missing or invalid dates in the 'Published At' column
            df['Published At'] = pd.to_datetime(df['Published At'], format='%Y-%m-%d %H:%M:%S', errors='coerce')
            df['Published At'].fillna(pd.to_datetime('1900-01-01 00:00:00'), inplace=True)

            # Sort the DataFrame by 'Published At' column in descending order
            df = df.sort_values(by='Published At', ascending=False)

            # Save the cleaned, sanitized, and sorted data to a new CSV file with UTF-8 encoding
            cleaned_csv_file_path = 'feed_data_cleaned.csv'
            df.to_csv(cleaned_csv_file_path, index=False, encoding='utf-8')

            logging.info('CSV file cleaned, sanitized, and sorted successfully.')
        except FileNotFoundError:
            logging.error('CSV file not found.')
        except Exception as e:
            logging.error(f'An error occurred: {str(e)}')
        "

    - name: Check for modified files
      id: git-check
      run: echo "modified=$(git status --porcelain | wc -l)" >> $GITHUB_OUTPUT

    - name: Commit and push changes
      if: steps.git-check.outputs.modified != '0'
      run: |
        git config --local user.email "${{ secrets.USER_EMAIL }}"
        git config --local user.name "${{ secrets.USER_NAME }}"
        git add feed_data_cleaned.csv
        git commit -m "Clean, sanitize, and sort CSV data"
        git push